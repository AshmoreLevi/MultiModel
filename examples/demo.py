"""
å¤šæ¨¡æ€é¡¹ç›®ç¤ºä¾‹ä»£ç 

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å¤„ç†å™¨å’Œå¤šæ¨¡æ€èåˆåŠŸèƒ½
"""

import numpy as np
import os
from multimodel import TextProcessor, ImageProcessor, AudioProcessor, MultimodalFusion


def demo_text_processing() -> None:
    """æ¼”ç¤ºæ–‡æœ¬å¤„ç†åŠŸèƒ½"""
    print("=== æ–‡æœ¬å¤„ç†æ¼”ç¤º ===")
    
    # åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨
    text_processor = TextProcessor()
    
    # ç¤ºä¾‹æ–‡æœ¬
    texts = [
        "è¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æœºå™¨å­¦ä¹ é¡¹ç›®çš„ç¤ºä¾‹æ–‡æœ¬ã€‚",
        "æˆ‘ä»¬å¯ä»¥å¤„ç†ä¸­æ–‡å’Œè‹±æ–‡æ–‡æœ¬ï¼Œæå–ç‰¹å¾è¿›è¡Œåˆ†æã€‚",
        "Text processing includes tokenization, feature extraction, and analysis."
    ]
    
    for i, text in enumerate(texts):
        print(f"\næ–‡æœ¬ {i+1}: {text}")
        
        # æ¸…ç†æ–‡æœ¬
        cleaned = text_processor.clean_text(text)
        print(f"æ¸…ç†å: {cleaned}")
        
        # ä¸­æ–‡åˆ†è¯
        tokens = text_processor.tokenize_chinese(text)
        print(f"åˆ†è¯ç»“æœ: {tokens}")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = text_processor.get_text_statistics(text)
        print(f"ç»Ÿè®¡ä¿¡æ¯: {stats}")
    
    # æ‰¹é‡å¤„ç†
    print("\n=== æ‰¹é‡æ–‡æœ¬å¤„ç† ===")
    batch_results = text_processor.batch_process(texts)
    print(f"æ‰¹é‡å¤„ç†å®Œæˆï¼Œå¤„ç†äº† {len(batch_results['cleaned_texts'])} ä¸ªæ–‡æœ¬")


def demo_image_processing() -> None:
    """æ¼”ç¤ºå›¾åƒå¤„ç†åŠŸèƒ½"""
    print("\n=== å›¾åƒå¤„ç†æ¼”ç¤º ===")
    
    # åˆå§‹åŒ–å›¾åƒå¤„ç†å™¨
    image_processor = ImageProcessor()
    
    # åˆ›å»ºç¤ºä¾‹å›¾åƒï¼ˆéšæœºå›¾åƒï¼‰
    print("åˆ›å»ºç¤ºä¾‹å›¾åƒæ•°æ®...")
    sample_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
    
    # é¢„å¤„ç†å›¾åƒ
    processed = image_processor.preprocess_image(sample_image)
    print(f"åŸå§‹å›¾åƒå½¢çŠ¶: {processed['original_shape']}")
    print(f"è°ƒæ•´åå½¢çŠ¶: {processed['resized'].shape}")
    
    # é¢œè‰²åˆ†æ
    color_analysis = image_processor.analyze_color(sample_image)
    print(f"ä¸»è¦é¢œè‰²æ•°é‡: {len(color_analysis.get('dominant_colors', []))}")
    
    # å¯¹è±¡æ£€æµ‹
    objects = image_processor.detect_objects(sample_image)
    print(f"æ£€æµ‹åˆ° {len(objects)} ä¸ªå¯¹è±¡")


def demo_audio_processing() -> None:
    """æ¼”ç¤ºéŸ³é¢‘å¤„ç†åŠŸèƒ½"""
    print("\n=== éŸ³é¢‘å¤„ç†æ¼”ç¤º ===")
    
    # åˆå§‹åŒ–éŸ³é¢‘å¤„ç†å™¨
    audio_processor = AudioProcessor()
    
    # åˆ›å»ºç¤ºä¾‹éŸ³é¢‘æ•°æ®ï¼ˆæ­£å¼¦æ³¢ï¼‰
    print("åˆ›å»ºç¤ºä¾‹éŸ³é¢‘æ•°æ®...")
    duration = 3  # 3ç§’
    sample_rate = 22050
    t = np.linspace(0, duration, int(sample_rate * duration))
    frequency = 440  # A4éŸ³ç¬¦
    sample_audio = np.sin(2 * np.pi * frequency * t)
    
    # é¢„å¤„ç†éŸ³é¢‘
    processed = audio_processor.preprocess_audio(sample_audio)
    print(f"éŸ³é¢‘æ—¶é•¿: {processed['duration']:.2f} ç§’")
    print(f"åŸå§‹é•¿åº¦: {processed['original_length']} æ ·æœ¬")
    
    # æå–éŸ³é¢‘ç‰¹å¾
    print("æå–éŸ³é¢‘ç‰¹å¾...")
    features = audio_processor.extract_comprehensive_features(sample_audio)
    
    if 'mfcc_mean' in features:
        print(f"MFCCç‰¹å¾ç»´åº¦: {len(features['mfcc_mean'])}")
    if 'tempo' in features:
        print(f"ä¼°è®¡èŠ‚æ‹: {features['tempo']:.1f} BPM")
    
    # éŸ³é¢‘è´¨é‡åˆ†æ
    if 'snr_estimate' in features:
        print(f"ä¿¡å™ªæ¯”ä¼°è®¡: {features['snr_estimate']:.2f} dB")


def demo_multimodal_fusion() -> None:
    """æ¼”ç¤ºå¤šæ¨¡æ€èåˆåŠŸèƒ½"""
    print("\n=== å¤šæ¨¡æ€èåˆæ¼”ç¤º ===")
    
    # åˆ›å»ºç¤ºä¾‹ç‰¹å¾
    text_features = np.random.randn(128)  # æ–‡æœ¬ç‰¹å¾
    image_features = np.random.randn(2048)  # å›¾åƒç‰¹å¾  
    audio_features = np.random.randn(256)  # éŸ³é¢‘ç‰¹å¾
    
    print(f"æ–‡æœ¬ç‰¹å¾ç»´åº¦: {text_features.shape}")
    print(f"å›¾åƒç‰¹å¾ç»´åº¦: {image_features.shape}")
    print(f"éŸ³é¢‘ç‰¹å¾ç»´åº¦: {audio_features.shape}")
    
    # æµ‹è¯•ä¸åŒçš„èåˆæ–¹æ³•
    fusion_methods = ["concatenation", "attention", "weighted", "pca"]
    
    for method in fusion_methods:
        print(f"\n--- {method.upper()} èåˆ ---")
        fusion = MultimodalFusion(fusion_method=method)
        
        if method == "weighted":
            weights = [0.4, 0.4, 0.2]  # æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘çš„æƒé‡
            result = fusion.fuse_modalities(
                text_features=text_features,
                image_features=image_features,
                audio_features=audio_features,
                weights=weights
            )
        else:
            result = fusion.fuse_modalities(
                text_features=text_features,
                image_features=image_features,
                audio_features=audio_features
            )
        
        if result.get("success"):
            print(f"èåˆæˆåŠŸï¼è¾“å‡ºå½¢çŠ¶: {result['output_shape']}")
        else:
            print(f"èåˆå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    # æ¨¡æ€è´¡çŒ®åº¦åˆ†æ
    print("\n--- æ¨¡æ€è´¡çŒ®åº¦åˆ†æ ---")
    fusion = MultimodalFusion()
    features_list = [text_features, image_features, audio_features]
    modality_names = ["text", "image", "audio"]
    
    contribution = fusion.analyze_modality_contribution(features_list, modality_names)
    
    for modality, stats in contribution.items():
        if isinstance(stats, dict) and "relative_importance" in stats:
            print(f"{modality}: ç›¸å¯¹é‡è¦æ€§ = {stats['relative_importance']:.3f}")


def demo_similarity_analysis() -> None:
    """æ¼”ç¤ºç›¸ä¼¼åº¦åˆ†æåŠŸèƒ½"""
    print("\n=== ç›¸ä¼¼åº¦åˆ†ææ¼”ç¤º ===")
    
    fusion = MultimodalFusion()
    
    # åˆ›å»ºä¸¤ç»„ç‰¹å¾ç”¨äºæ¯”è¾ƒ
    features_a = np.random.randn(100)
    features_b = np.random.randn(100)
    features_c = features_a + np.random.randn(100) * 0.1  # ä¸Aç›¸ä¼¼çš„ç‰¹å¾
    
    # è®¡ç®—ä¸åŒç›¸ä¼¼åº¦
    similarity_methods = ["cosine", "euclidean", "pearson"]
    
    for method in similarity_methods:
        sim_ab = fusion.compute_similarity(features_a, features_b, method)
        sim_ac = fusion.compute_similarity(features_a, features_c, method)
        
        print(f"{method.upper()} ç›¸ä¼¼åº¦:")
        print(f"  A vs B: {sim_ab:.4f}")
        print(f"  A vs C: {sim_ac:.4f}")


def create_sample_data() -> None:
    """åˆ›å»ºç¤ºä¾‹æ•°æ®æ–‡ä»¶"""
    print("\n=== åˆ›å»ºç¤ºä¾‹æ•°æ® ===")
    
    # åˆ›å»ºæ•°æ®ç›®å½•
    data_dir = "sample_data"
    os.makedirs(data_dir, exist_ok=True)
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æœ¬æ–‡ä»¶
    texts = [
        "è¿™æ˜¯ç¬¬ä¸€ä¸ªç¤ºä¾‹æ–‡æœ¬ï¼ŒåŒ…å«ä¸­æ–‡å†…å®¹ã€‚",
        "This is the second sample text in English.",
        "ç¬¬ä¸‰ä¸ªæ–‡æœ¬æ··åˆäº†ä¸­æ–‡å’ŒEnglish contentã€‚"
    ]
    
    with open(os.path.join(data_dir, "sample_texts.txt"), "w", encoding="utf-8") as f:
        for i, text in enumerate(texts):
            f.write(f"Text {i+1}: {text}\n")
    
    print(f"ç¤ºä¾‹æ•°æ®å·²åˆ›å»ºåœ¨ {data_dir} ç›®å½•ä¸­")


def main() -> None:
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("ğŸš€ å¤šæ¨¡æ€æœºå™¨å­¦ä¹ é¡¹ç›®æ¼”ç¤º")
    print("=" * 50)
    
    try:
        # æ–‡æœ¬å¤„ç†æ¼”ç¤º
        demo_text_processing()
        
        # å›¾åƒå¤„ç†æ¼”ç¤º
        demo_image_processing()
        
        # éŸ³é¢‘å¤„ç†æ¼”ç¤º
        demo_audio_processing()
        
        # å¤šæ¨¡æ€èåˆæ¼”ç¤º
        demo_multimodal_fusion()
        
        # ç›¸ä¼¼åº¦åˆ†ææ¼”ç¤º
        demo_similarity_analysis()
        
        # åˆ›å»ºç¤ºä¾‹æ•°æ®
        create_sample_data()
        
        print("\nğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ æç¤º:")
        print("- å®‰è£…ä¾èµ–: pip install -r requirements.txt")
        print("- è¿è¡Œæ¼”ç¤º: python examples/demo.py")
        print("- æŸ¥çœ‹æ–‡æ¡£: è¯·å‚è€ƒREADME.md")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–åŒ…")


if __name__ == "__main__":
    main()
